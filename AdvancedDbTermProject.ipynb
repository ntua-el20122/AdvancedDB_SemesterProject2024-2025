{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a40df-2b08-452d-9a9d-3a0b9d9642e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Δημιουργία SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"All_Queries\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea53da41-15d1-4bb9-9ff8-e4627dbd8ab1",
   "metadata": {},
   "source": [
    "### Το ακόλουθο configuration για τα Queries 1 και 2 και 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56039f99-0444-4e10-8e35-f38f69e2e92f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "    \"spark.executor.instances\": \"4\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93fee96-7160-4716-8a79-dc0451236fa9",
   "metadata": {},
   "source": [
    "### Το ακόλουθο configuration για τo Query 4α)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f7ee96-e189-430a-b627-9ba2c906e79e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "    \"spark.executor.instances\": \"2\",\n",
    "    \"spark.executor.cores\": \"1\",\n",
    "     \"spark.executor.memory\": \"2g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad19560-fa0f-4a45-914c-7c51249354fd",
   "metadata": {},
   "source": [
    "### Το ακόλουθο configuration για τo Query 4β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abad935-61cd-4184-994b-e4d573262929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "    \"spark.executor.instances\": \"2\",\n",
    "    \"spark.executor.cores\": \"2\",\n",
    "     \"spark.executor.memory\": \"4g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f0c238-cd58-4d02-9b04-fec2670e72b6",
   "metadata": {},
   "source": [
    "### Το ακόλουθο configuration για τo Query 4γ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a823b-cbdd-4e96-95ef-e148fd2555e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "    \"spark.executor.instances\": \"2\",\n",
    "    \"spark.executor.cores\": \"4\",\n",
    "     \"spark.executor.memory\": \"8g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d85a18-49f1-46f3-9f0b-4cd3b9cdb8aa",
   "metadata": {},
   "source": [
    "### Το ακόλουθο configuration για τo Query 5α)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee710a7c-f528-4f07-ab93-6a4799753a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.cores\": \"4\",\n",
    "        \"spark.executor.memory\": \"8g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01c3089-0339-40e7-9f22-2787acc52ce6",
   "metadata": {},
   "source": [
    "### Το ακόλουθο configuration για τo Query 5β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3826c28e-5b5f-495a-a196-d8251fd0f8dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"4\",\n",
    "        \"spark.executor.cores\": \"2\",\n",
    "        \"spark.executor.memory\": \"4g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6b2aa1-61ed-4b7a-9f80-f17a63f8052c",
   "metadata": {},
   "source": [
    "### Το ακόλουθο configuration για τo Query 5γ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6e5bbe-582f-4f50-b960-3d1acf44d4a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"8\",\n",
    "        \"spark.executor.cores\": \"1\",\n",
    "        \"spark.executor.memory\": \"2g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b6744a-7b22-4e86-959c-95d1b7ea15c7",
   "metadata": {},
   "source": [
    "### Query 1 με χρήση DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca03d33-59cb-4cc9-a048-30dacba1bb9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, desc\n",
    "\n",
    "\n",
    "# Φόρτωση δεδομένων\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "data1 = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "data2 = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Ένωση των δύο DataFrames\n",
    "data = data1.union(data2)\n",
    "\n",
    "\n",
    "# Φιλτράρισμα για \"aggravated assault\"\n",
    "filtered = data.filter(col(\"Crm Cd Desc\").contains(\"AGGRAVATED ASSAULT\"))\n",
    "\n",
    "# Κατηγοριοποίηση σε ηλικιακές ομάδες\n",
    "grouped = filtered.withColumn(\n",
    "    \"AgeGroup\",\n",
    "    when((col(\"Vict Age\") < 18) & (col(\"Vict Age\") > 0), \"Children\")\n",
    "    .when((col(\"Vict Age\") >= 18) & (col(\"Vict Age\") <= 24), \"Young Adults\")\n",
    "    .when((col(\"Vict Age\") >= 25) & (col(\"Vict Age\") <= 64), \"Adults\")\n",
    "    .when(col(\"Vict Age\") > 64, \"Elderly\")\n",
    "    .when(col(\"Vict Age\") <= 0, \"Invalid Age\")\n",
    ")\n",
    "\n",
    "# Ομαδοποίηση και καταμέτρηση\n",
    "result_df = grouped.groupBy(\"AgeGroup\").count().orderBy(desc(\"count\"))\n",
    "\n",
    "# Εμφάνιση αποτελεσμάτων\n",
    "result_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Καταγραφή χρόνου\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "elapsed_time_row = spark.createDataFrame([(\"Elapsed Time\", elapsed_time)], [\"AgeGroup\", \"count\"])\n",
    "result_with_time = result_df.union(elapsed_time_row)\n",
    "\n",
    "#Αποθήκευση του αποτελέσματος\n",
    "output_path = \"s3://groups-bucket-dblab-905418150721/group37/query1_DataFrame_result_and_time\"\n",
    "\n",
    "result_with_time.write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Execution time (DataFrame API): {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84911bef-0585-4c71-b9c3-b6c621790e00",
   "metadata": {},
   "source": [
    "### Query 1 χρησιμοποιώντας RDD (filter, map, reduceByKey και sortBy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c012a0-58fc-418c-ac36-b667a9908e80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Φόρτωση δεδομένων\n",
    "data1 = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True).rdd\n",
    "data2 = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", header=True, inferSchema=True).rdd\n",
    "\n",
    "# Ένωση των δύο RDDs\n",
    "data = data1.union(data2)\n",
    "\n",
    "\n",
    "# Φιλτράρισμα για \"aggravated assault\"\n",
    "filtered = data.filter(lambda row: \"AGGRAVATED ASSAULT\" in str(row[\"Crm Cd Desc\"]))\n",
    "\n",
    "# Κατηγοριοποίηση σε ηλικιακές ομάδες\n",
    "def categorize_age(row):\n",
    "    age = row[\"Vict Age\"]\n",
    "    if age is None:  # Έλεγχος για null τιμές\n",
    "        return None\n",
    "    if age < 18 and age > 0:\n",
    "        return \"Children\"\n",
    "    elif 18 <= age <= 24:\n",
    "        return \"Young Adults\"\n",
    "    elif 25 <= age <= 64:\n",
    "        return \"Adults\"\n",
    "    elif age > 64:\n",
    "        return \"Elderly\"\n",
    "    else:\n",
    "        return \"Invalid Age\"\n",
    "\n",
    "\n",
    "# Δημιουργία RDD με κατηγοριοποιημένες ηλικίες\n",
    "age_group_rdd = filtered.map(lambda row: (categorize_age(row), 1))\n",
    "\n",
    "# Αφαίρεση null τιμών\n",
    "age_group_rdd = age_group_rdd.filter(lambda x: x[0] is not None)\n",
    "\n",
    "# Ομαδοποίηση και Καταμέτρηση\n",
    "result_rdd = age_group_rdd.reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "# Εμφάνιση αποτελεσμάτων\n",
    "for group, count in result_rdd.collect():\n",
    "    print(f\"{group}: {count}\")\n",
    "\n",
    "end_time = time.time()\n",
    "    \n",
    "# Μετατροπή του αποτελέσματος σε DataFrame\n",
    "result_df = spark.createDataFrame(result_rdd, [\"AgeGroup\", \"count\"])\n",
    "\n",
    "# Προσθήκη του χρόνου εκτέλεσης ως νέα γραμμή\n",
    "elapsed_time = end_time - start_time\n",
    "elapsed_time_row = spark.createDataFrame([(\"Elapsed Time\", elapsed_time)], [\"AgeGroup\", \"count\"])\n",
    "result_with_time = result_df.union(elapsed_time_row)\n",
    "\n",
    "# Αποθήκευση του αποτελέσματος\n",
    "output_path = \"s3://groups-bucket-dblab-905418150721/group37/query1_MapReduce_result_and_time\"\n",
    "result_with_time.write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n",
    "\n",
    "# Εμφάνιση αποτελεσμάτων\n",
    "print(f\"Execution time (RDD API): {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13f50d7-88ae-4f7a-8537-72a5c7349329",
   "metadata": {},
   "source": [
    "RDD vs DataFrame \n",
    "\n",
    "Η DataFrame API είναι πιο γρήγορη από την RDD API στη συγκεκριμένη περίπτωση λόγω της αρχιτεκτονικής και των χαρακτηριστικών του Spark.\n",
    "\n",
    "Catalyst Optimizer\n",
    "\n",
    "Η DataFrame API αξιοποιεί τον Catalyst Optimizer, έναν ισχυρό βελτιστοποιητή ερωτημάτων που:\n",
    "\n",
    "    Εφαρμόζει βελτιστοποιήσεις λογικών και φυσικών σχεδίων εκτέλεσης.\n",
    "    Αναδιοργανώνει τις λειτουργίες (π.χ., φιλτράρισμα, ομαδοποίηση) για να ελαχιστοποιήσει το κόστος εκτέλεσης.\n",
    "    Εκμεταλλεύεται ευκαιρίες για predicate pushdown, μειώνοντας τα δεδομένα που μεταφέρονται ή υποβάλλονται σε επεξεργασία.\n",
    "\n",
    "Στην περίπτωσή μας:\n",
    "\n",
    "    Το φιλτράρισμα (filter) για Crm Cd Desc και η ομαδοποίηση (groupBy) υλοποιούνται αποδοτικά, χωρίς περιττές λειτουργίες.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22b9d7-9c34-4f0b-8ba6-3c492b74a794",
   "metadata": {},
   "source": [
    "### Query 2 α) (DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a19691-323f-4bb4-8af4-b9f6f1429d6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, count, sum, when, to_date, to_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import time\n",
    "\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Φόρτωση δεδομένων ως DataFrames\n",
    "data1 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "data2 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Ένωση των δύο DataFrames\n",
    "data = data1.union(data2)\n",
    "\n",
    "# Correctly parse DATE OCC and extract the year\n",
    "data = data.withColumn(\"DATE OCC\", to_date(to_timestamp(col(\"DATE OCC\"), \"MM/dd/yyyy hh:mm:ss a\")))\n",
    "data = data.withColumn(\"Year\", year(col(\"DATE OCC\")))\n",
    "\n",
    "\n",
    "\n",
    "# Φιλτράρισμα για ανοικτές και κλεισμένες υποθέσεις\n",
    "data = data.withColumn(\n",
    "    \"is_closed_case\",\n",
    "    when(~col(\"Status Desc\").isin(\"UNK\", \"Invest Cont\"), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Υπολογισμός συνολικών και κλεισμένων υποθέσεων ανά τμήμα και έτος\n",
    "aggregated = data.groupBy(\"Year\", \"AREA NAME\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_cases\"),\n",
    "        sum(col(\"is_closed_case\")).alias(\"closed_cases\")\n",
    "    ) \\\n",
    "    .withColumn(\"closed_case_rate\", (col(\"closed_cases\") / col(\"total_cases\")) * 100)\n",
    "\n",
    "# Υπολογισμός κατάταξης για κάθε τμήμα ανά έτος\n",
    "windowSpec = Window.partitionBy(\"Year\").orderBy(col(\"closed_case_rate\").desc())\n",
    "aggregated = aggregated.withColumn(\"#\", dense_rank().over(windowSpec))\n",
    "\n",
    "# Επιλογή των top 3 τμημάτων ανά έτος\n",
    "result = aggregated.filter(col(\"#\") <= 3).orderBy([\"Year\", \"#\"])\n",
    "\n",
    "# Επιλογή των απαραίτητων στηλών και μετονομασία για καλύτερη αναγνωσιμότητα\n",
    "final_result = result.select(\n",
    "    col(\"Year\").alias(\"year\"),\n",
    "    col(\"AREA NAME\").alias(\"precinct\"),\n",
    "    col(\"closed_case_rate\").alias(\"closed_case_rate\"),\n",
    "    col(\"#\").alias(\"#\")\n",
    ")\n",
    "\n",
    "# Εμφάνιση του τελικού αποτελέσματος\n",
    "final_result.show(truncate=False)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Υπολογισμός χρόνου εκτέλεσης\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Δημιουργία DataFrame για τον χρόνο εκτέλεσης\n",
    "execution_time_schema = StructType([\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"precinct\", StringType(), True),\n",
    "    StructField(\"closed_case_rate\", DoubleType(), True),\n",
    "    StructField(\"#\", StringType(), True)\n",
    "])\n",
    "execution_time_df = spark.createDataFrame(\n",
    "    [(\"Execution Time\", None, execution_time, None)],\n",
    "    schema=execution_time_schema\n",
    ")\n",
    "\n",
    "# Προσθήκη του χρόνου εκτέλεσης στο αποτέλεσμα\n",
    "final_result_with_time = final_result.union(execution_time_df)\n",
    "\n",
    "\n",
    "\n",
    "# Save the final result with elapsed time to a CSV file\n",
    "output_path = \"s3://groups-bucket-dblab-905418150721/group37/query2_DataFrame\"\n",
    "final_result_with_time.write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n",
    "\n",
    "# Print the time taken\n",
    "print(f\"Query 2 execution time using DataFrame: {execution_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3d583-d00a-4418-8dd9-276876eca19c",
   "metadata": {},
   "source": [
    "Query 2 α) (SQL API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762726f4-3900-4bd0-b3db-4e853752f94f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, to_timestamp, year\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import time\n",
    "\n",
    "# Set the legacy time parser policy\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "\n",
    "\n",
    "# Φόρτωση δεδομένων ως DataFrames\n",
    "data1 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "data2 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Ένωση των δύο DataFrames\n",
    "data = data1.union(data2)\n",
    "\n",
    "# Correctly parse DATE OCC and extract the year\n",
    "data = data.withColumn(\"DATE OCC\", to_date(to_timestamp(col(\"DATE OCC\"), \"MM/dd/yyyy hh:mm:ss a\")))\n",
    "data = data.withColumn(\"Year\", year(col(\"DATE OCC\")))\n",
    "\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a temporary SQL view\n",
    "data.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "# SQL Query\n",
    "query = \"\"\"\n",
    "WITH processed_data AS (\n",
    "    SELECT\n",
    "        `Year` AS year,\n",
    "        `AREA NAME` AS precinct,\n",
    "        COUNT(*) AS total_cases,\n",
    "        SUM(CASE WHEN `Status Desc` NOT IN ('UNK', 'Invest Cont') THEN 1 ELSE 0 END) AS closed_cases,\n",
    "        (SUM(CASE WHEN `Status Desc` NOT IN ('UNK', 'Invest Cont') THEN 1 ELSE 0 END) * 100.0 / COUNT(*)) AS closed_case_rate\n",
    "    FROM crime_data\n",
    "    GROUP BY `Year`, `AREA NAME`\n",
    "),\n",
    "ranked_data AS (\n",
    "    SELECT\n",
    "        year,\n",
    "        precinct,\n",
    "        closed_case_rate,\n",
    "        ROW_NUMBER() OVER (PARTITION BY year ORDER BY closed_case_rate DESC) AS rank\n",
    "    FROM processed_data\n",
    ")\n",
    "SELECT \n",
    "    year,\n",
    "    precinct,\n",
    "    closed_case_rate,\n",
    "    rank AS `#`\n",
    "FROM ranked_data\n",
    "WHERE rank <= 3\n",
    "ORDER BY year ASC, rank ASC\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "result = spark.sql(query)\n",
    "\n",
    "\n",
    "result.show()\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate execution time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Add execution time to the result\n",
    "execution_time_schema = StructType([\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"precinct\", StringType(), True),\n",
    "    StructField(\"closed_case_rate\", DoubleType(), True),\n",
    "    StructField(\"#\", StringType(), True)\n",
    "])\n",
    "\n",
    "execution_time_row = spark.createDataFrame(\n",
    "    [(\"Execution Time\", None, execution_time, None)],\n",
    "    schema=execution_time_schema\n",
    ")\n",
    "\n",
    "# Append the execution time row to the result\n",
    "final_result = result.union(execution_time_row)\n",
    "\n",
    "# Save the final result to a CSV file\n",
    "output_path = \"s3://groups-bucket-dblab-905418150721/group37/query2_SQL\"\n",
    "final_result.write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n",
    "\n",
    "# Print the time taken\n",
    "print(f\"Query 2 execution time using SQL_API: {execution_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b1b4e8-5862-4426-9c34-f274ce247f9f",
   "metadata": {},
   "source": [
    "Στην πράξη, το SQL API είναι πιο γρήγορο από το DataFrame API για ερωτήματα που εκφράζονται φυσικά μέσω SQL, όπως φιλτραρίσματα, ενώσεις (joins) και ομαδοποιήσεις (groupBy). Αυτό συμβαίνει επειδή το SQL API μπορεί να εκμεταλλευτεί πληρέστερα τον Catalyst Optimizer, μειώνοντας την επεξεργασία δεδομένων και τις λειτουργίες ανάγνωσης (I/O). Το DataFrame API, αν και βελτιστοποιημένο, ενδέχεται να περιλαμβάνει περιττά στάδια επεξεργασίας αν οι λειτουργίες δεν περιγράφονται με σαφήνεια.\n",
    "\n",
    "Επιπλέον, το SQL API είναι πιο εύκολο να διαβαστεί και να διατηρηθεί για όσους έχουν ήδη εμπειρία στη SQL. Παρέχει επίσης καλύτερη υποστήριξη εργαλείων τρίτων για την παρακολούθηση και την ανάλυση ερωτημάτων. Ωστόσο, το DataFrame API είναι καλύτερο για πιο σύνθετες λογικές, όπως υπολογισμοί σε πολλαπλά στάδια, ενσωμάτωση εξωτερικών δεδομένων ή χρήση συνδυασμένων λειτουργιών προγραμματισμού.\n",
    "\n",
    "Συμπερασματικά, το SQL API είναι η προτιμότερη επιλογή για δηλωτικές ερωτήσεις με σταθερή δομή και έμφαση στην απόδοση, ενώ το DataFrame API είναι ιδανικό για πιο σύνθετες και ευέλικτες εργασίες δεδομένων. Η επιλογή μεταξύ των δύο εξαρτάται από τις απαιτήσεις του έργου και την εμπειρία του χρήστη."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac982b3c-dea0-489a-a4eb-e94ff50145f7",
   "metadata": {},
   "source": [
    "### Query 2 β) Εκτέλεση για Parquet Format με DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f6cfa-7ae7-4458-b556-02382d662d47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, count, sum, when, to_date, to_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import time\n",
    "\n",
    "# Set the legacy time parser policy\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Load CSV data as DataFrames\n",
    "data1 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "data2 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Combine the two DataFrames\n",
    "data = data1.union(data2)\n",
    "\n",
    "# Save combined data in Parquet format\n",
    "output_path = \"s3://groups-bucket-dblab-905418150721/group37/CrimeData_parquet_ReExecution\"\n",
    "data.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "\n",
    "# Start timing optimized query\n",
    "start_optimized_parquet_query = time.time()\n",
    "\n",
    "# Load Parquet data\n",
    "parquet_data = spark.read.parquet(output_path)\n",
    "\n",
    "# Select relevant columns for processing\n",
    "selected_data = parquet_data.select(\"DATE OCC\", \"AREA NAME\", \"Status Desc\")\n",
    "\n",
    "# Filter and preprocess data\n",
    "filtered_data = selected_data.withColumn(\n",
    "    \"DATE OCC\", to_date(to_timestamp(col(\"DATE OCC\"), \"MM/dd/yyyy hh:mm:ss a\"))\n",
    ").withColumn(\n",
    "    \"Year\", year(col(\"DATE OCC\"))\n",
    ").filter(col(\"Status Desc\").isNotNull())\n",
    "\n",
    "# Add a column to indicate if a case is closed\n",
    "filtered_data = filtered_data.withColumn(\n",
    "    \"is_closed_case\",\n",
    "    when(~col(\"Status Desc\").isin(\"UNK\", \"Invest Cont\"), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Partition data by Year for efficient parallel processing\n",
    "partitioned_data = filtered_data.repartition(\"Year\")\n",
    "\n",
    "# Aggregate data\n",
    "aggregated_data = partitioned_data.groupBy(\"Year\", \"AREA NAME\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_cases\"),\n",
    "        sum(col(\"is_closed_case\")).alias(\"closed_cases\")\n",
    "    ) \\\n",
    "    .withColumn(\"closed_case_rate\", (col(\"closed_cases\") / col(\"total_cases\")) * 100)\n",
    "\n",
    "# Use window function for ranking precincts\n",
    "windowSpec = Window.partitionBy(\"Year\").orderBy(col(\"closed_case_rate\").desc())\n",
    "ranked_data = aggregated_data.withColumn(\"#\", dense_rank().over(windowSpec))\n",
    "\n",
    "# Filter top 3 precincts per year and select relevant columns\n",
    "result = ranked_data.filter(col(\"#\") <= 3).orderBy([\"Year\", \"#\"]).select(\n",
    "    col(\"Year\").alias(\"year\"),\n",
    "    col(\"AREA NAME\").alias(\"precinct\"),\n",
    "    col(\"closed_case_rate\").alias(\"closed_case_rate\"),\n",
    "    col(\"#\").alias(\"#\")\n",
    ")\n",
    "\n",
    "\n",
    "result.show()\n",
    "\n",
    "# End timing\n",
    "end_optimized_parquet_query = time.time()\n",
    "\n",
    "# Add execution time as a new row\n",
    "execution_time = end_optimized_parquet_query - start_optimized_parquet_query\n",
    "\n",
    "# Define the schema explicitly for the execution time DataFrame\n",
    "execution_time_schema = StructType([\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"precinct\", StringType(), True),\n",
    "    StructField(\"closed_case_rate\", DoubleType(), True),\n",
    "    StructField(\"#\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create the execution time DataFrame with the schema\n",
    "execution_time_df = spark.createDataFrame(\n",
    "    [(\"Execution Time\", None, execution_time, None)],\n",
    "    schema=execution_time_schema\n",
    ")\n",
    "\n",
    "# Append execution time to the result DataFrame\n",
    "final_result = result.union(execution_time_df)\n",
    "\n",
    "\n",
    "# Save final result to CSV\n",
    "final_output_path = \"s3://groups-bucket-dblab-905418150721/group37/query2_optimized_results\"\n",
    "final_result.write.mode(\"overwrite\").option(\"header\", \"true\").csv(final_output_path)\n",
    "\n",
    "# Print execution time\n",
    "print(f\"Parquet query execution time: {execution_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33601c79-08ee-4ff7-a17e-6e47dd933933",
   "metadata": {},
   "source": [
    "Το Parquet είναι πιο γρήγορο και αποδοτικό από το CSV για την ανάλυση δεδομένων σε Spark, εξαιτίας του τρόπου αποθήκευσης και ανάγνωσης των δεδομένων. Το Parquet αποθηκεύει τα δεδομένα σε στηλοθετημένη μορφή (columnar storage), κάτι που επιτρέπει στο Spark να διαβάζει μόνο τις στήλες που είναι απαραίτητες για ένα ερώτημα, μειώνοντας την I/O επιβάρυνση. Αντίθετα, το CSV αποθηκεύει τα δεδομένα ανά γραμμή (row-based storage), αναγκάζοντας το Spark να διαβάσει ολόκληρες γραμμές, ακόμα και αν χρειάζονται μόνο συγκεκριμένες στήλες.\n",
    "\n",
    "Επιπλέον, το Parquet υποστηρίζει το λεγόμενο predicate pushdown, μια τεχνική που εφαρμόζει φίλτρα, όπως ένα WHERE ερώτημα, απευθείας στο επίπεδο αποθήκευσης. Αυτό έχει ως αποτέλεσμα τη μείωση του όγκου δεδομένων που διαβάζεται και επεξεργάζεται στη μνήμη. Από την άλλη πλευρά, το CSV δεν έχει αυτή τη δυνατότητα, με αποτέλεσμα όλα τα δεδομένα να φορτώνονται πρώτα και να φιλτράρονται αργότερα, κάτι που αυξάνει τον χρόνο εκτέλεσης και τη χρήση της μνήμης.\n",
    "\n",
    "Το Parquet χρησιμοποιεί επίσης αποδοτικούς αλγορίθμους συμπίεσης, όπως Snappy ή GZIP, για να μειώσει το μέγεθος αποθήκευσης των δεδομένων και να μειώσει την I/O επιβάρυνση κατά την ανάγνωση. Σε αντίθεση με αυτό, τα αρχεία CSV είναι συνήθως μη συμπιεσμένα, αυξάνοντας τόσο το μέγεθος όσο και τον χρόνο που απαιτείται για την ανάγνωση. Ένα άλλο σημαντικό πλεονέκτημα του Parquet είναι ότι περιλαμβάνει ενσωματωμένο σχήμα (schema information), κάτι που επιτρέπει την ταχύτερη ανάγνωση των δεδομένων χωρίς την ανάγκη ανάλυσης του σχήματος (schema inference). Αντίθετα, το Spark πρέπει να αναγνωρίσει το σχήμα των δεδομένων σε ένα CSV αρχείο, κάτι που είναι χρονοβόρο για μεγάλα datasets.\n",
    "\n",
    "Το Parquet επιτρέπει επίσης αποδοτική παράλληλη ανάγνωση των δεδομένων σε κατανεμημένα συστήματα, αξιοποιώντας καλύτερα τους πόρους του cluster. Το CSV, αν και υποστηρίζει παράλληλη ανάγνωση, είναι λιγότερο αποδοτικό λόγω του τρόπου αποθήκευσης των δεδομένων σε γραμμές. Επίσης, το Parquet αποθηκεύει σημαντικά μεταδεδομένα, όπως τις ελάχιστες και μέγιστες τιμές για κάθε στήλη, επιτρέποντας στο Spark να παραλείπει δεδομένα που δεν είναι σχετικά με ένα ερώτημα. Το CSV, από την άλλη πλευρά, δεν περιλαμβάνει μεταδεδομένα, αναγκάζοντας το Spark να διαβάσει ολόκληρο το αρχείο.\n",
    "\n",
    "Συνολικά, το Parquet είναι ιδανικό για την ανάλυση μεγάλων δεδομένων, προσφέροντας σημαντικά πλεονεκτήματα σε ταχύτητα, αποδοτικότητα και μείωση της I/O επιβάρυνσης. Το Parquet είναι ειδικά σχεδιασμένο για συστήματα κατανεμημένης επεξεργασίας, όπως το Spark, καθιστώντας το την καλύτερη επιλογή για την αποθήκευση και επεξεργασία μεγάλων datasets σε σύγκριση με το CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d0eec-e509-498e-8461-db9fd9845019",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query 3 με χρήση DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75426e7-be19-4cec-b548-b1cc1fa49eed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import sys\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col, regexp_replace, sum, round, desc, count\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "\n",
    "def capture_explain_output(df):\n",
    "    \"\"\"\n",
    "    Calls df.explain() but captures its stdout text in a StringIO\n",
    "    and returns that string without printing to console.\n",
    "    \"\"\"\n",
    "    buffer = io.StringIO()\n",
    "    old_stdout = sys.stdout\n",
    "    try:\n",
    "        sys.stdout = buffer\n",
    "        df.explain(mode=\"formatted\")\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "    return buffer.getvalue()\n",
    "\n",
    "\n",
    "def query_3(join_strategy=\"\", join_explain=True):\n",
    "    \"\"\"\n",
    "    join_strategy: optional. Καθορίζει με ποια στρατηγική θα γίνει το join.\n",
    "    Έχει default τιμη \"\". Πιθανές Τιμές:\n",
    "    \"BROADCAST\", \"MERGE\", \"SHUFFLE_HASH\", \"SHUFFLE_REPLICATE_NL\"\n",
    "    \n",
    "    join_explain: optional. Είναι boolean και έχει default τιμή True.\n",
    "    Εάν είναι True, τότε εκτυπώνονται λεπτομέρειες σχετικά με το join strategy που ακολουθήθηκε\n",
    "    \"\"\"\n",
    "    \n",
    "    if join_strategy == \"\":\n",
    "        strategy = \"DEFAULT\"\n",
    "    else:\n",
    "        strategy = join_strategy\n",
    "\n",
    "    # Create sedona context\n",
    "    sedona = SedonaContext.create(spark)\n",
    "    \n",
    "    # Διαβάζουμε τα δεδομένα της απογραφής πληθυσμού (geojson)\n",
    "    geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "    blocks_df = (\n",
    "        sedona.read\n",
    "        .format(\"geojson\")\n",
    "        .option(\"multiLine\", \"true\")\n",
    "        .load(geojson_path)\n",
    "        .selectExpr(\"explode(features) as features\")\n",
    "        .select(\"features.*\")\n",
    "    )\n",
    "\n",
    "    # Formatting magic - Flattening\n",
    "    flattened_df = (\n",
    "        blocks_df.select(\n",
    "            [\n",
    "                col(f\"properties.{col_name}\").alias(col_name)\n",
    "                for col_name in blocks_df.schema[\"properties\"].dataType.fieldNames()\n",
    "            ]\n",
    "            + [\"geometry\"]\n",
    "        )\n",
    "        .drop(\"properties\")\n",
    "        .drop(\"type\")\n",
    "    )\n",
    "\n",
    "    # Διαβάζουμε τα δεδομένα με το μέσο εισόδημα νοικοκυριού\n",
    "    income_df = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .options(header=\"true\", inferSchema=\"true\", quote='\"', escape='\"')\n",
    "        .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\")\n",
    "    )\n",
    "\n",
    "    # Αφαιρούμε τα σύμβολα $ και , από την στήλη 'Estimated Median Income'\n",
    "    income_df = income_df.withColumn(\n",
    "        \"Estimated Median Income\",\n",
    "        regexp_replace(col(\"Estimated Median Income\"), r\"\\$\", \"\")\n",
    "    )\n",
    "    income_df = income_df.withColumn(\n",
    "        \"Estimated Median Income\",\n",
    "        regexp_replace(col(\"Estimated Median Income\"), r\",\", \"\")\n",
    "    )\n",
    "\n",
    "    # Μετατρέπουμε την στήλη 'Estimated Median Income' από string σε decimal\n",
    "    income_df = income_df.withColumn(\n",
    "        \"Estimated Median Income\",\n",
    "        col(\"Estimated Median Income\").cast(DecimalType())\n",
    "    )\n",
    "\n",
    "    ########################## Υπολογισμός μέσου ετήσιου εισοδήματος ανά άτομο ########################\n",
    "    \n",
    "    # Αφαιρούμε τις περιοχές που έχουν < 0 κατοίκους και κρατάμε\n",
    "    # μόνο τις περιοχές που ανήκουν στην πόλη Los Angeles\n",
    "    non_zero = flattened_df.filter(\n",
    "        ((flattened_df.HOUSING10 >= 0) & (flattened_df.POP_2010 >= 0))\n",
    "        & (flattened_df.CITY == \"Los Angeles\")\n",
    "    )\n",
    "    \n",
    "    # Ομαδοποιούμε κατά (COMM, ZCTA10) και βρίσκουμε για κάθε (COMM, ZCTA10)\n",
    "    # το συνολικό αριθμό νοικοκυριών (total_houses) και κατοίκων (total_pop)\n",
    "    population_agg = (\n",
    "        non_zero\n",
    "        .groupBy([non_zero.COMM, non_zero.ZCTA10])\n",
    "        .agg(\n",
    "            sum(\"HOUSING10\").alias(\"total_houses\"),\n",
    "            sum(\"POP_2010\").alias(\"total_pop\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Inner join μεταξύ population_agg και income_df βάσει του zip code\n",
    "    # Με αυτόν τον τρόπο, για κάθε ζεύγος (COMM, zip_code), υπολογίζουμε\n",
    "    # το γινόμενο total_houses * Estimated Median Income. Έτσι, βρίσκουμε το άθροισμα\n",
    "    # όλων των εισοδημάτων των κατοίκων για κάθε ζεύγος (COMM, zip_code).\n",
    "    if join_strategy == \"\":\n",
    "        income_hint = income_df\n",
    "    else:\n",
    "        income_hint = income_df.hint(join_strategy)\n",
    "\n",
    "    start_time = time.time()    # Start timer\n",
    "    comm_income = (\n",
    "        population_agg\n",
    "        .join(income_df.hint(join_strategy), population_agg.ZCTA10 == income_df[\"Zip Code\"], \"inner\")\n",
    "        .select(\n",
    "            population_agg.ZCTA10,\n",
    "            population_agg.COMM,\n",
    "            population_agg.total_pop,\n",
    "            (population_agg.total_houses * income_df[\"Estimated Median Income\"]).alias(\"total_money\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    comm_income.show()\n",
    "    end_time = time.time()     # Stop timer   \n",
    "    elapsed_time = end_time - start_time\n",
    "        \n",
    "    # Με τη μέθοδο explain() βλέπουμε πληροφορίες για την στρατηγική του join\n",
    "    if join_explain:\n",
    "        execution_plan = capture_explain_output(comm_income)\n",
    "        \n",
    "        # Αποθηκεύουμε το χρόνο εκτ΄έλεσης και το execution plan σε ένα CSV\n",
    "        output_path = f\"s3://groups-bucket-dblab-905418150721/group37/query3_{strategy}/First_Join\"\n",
    "        spark.createDataFrame([(strategy, elapsed_time, execution_plan)],\n",
    "                             [\"Join Strategy\", \"Elapsed Time (sec)\", \"Execution Plan\"]) \\\n",
    "                            .coalesce(1) \\\n",
    "                            .write \\\n",
    "                            .mode(\"overwrite\") \\\n",
    "                            .option(\"header\", \"true\") \\\n",
    "                            .csv(output_path)\n",
    "        \n",
    "    # Για κάθε COMM, υπολογίζουμε το άθροισμα των κατοίκων και\n",
    "    # το άθροισμα όλων των εισοδημάτων.\n",
    "    comm_income = (\n",
    "        comm_income\n",
    "        .groupBy(comm_income.COMM)\n",
    "        .agg(\n",
    "            sum(\"total_money\").alias(\"total_money\"),\n",
    "            sum(\"total_pop\").alias(\"total_pop\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ################ Υπολογισμός αναλογίας συνολικού αριθμού εγκλημάτων ανά άτομο ################\n",
    "\n",
    "    # Διαβάζουμε τα δεδομένα με τα εγκλήματα 2010 - 2024\n",
    "    crime_data = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .options(header=\"true\", inferSchema=\"true\", quote='\"', escape='\"')\n",
    "        .load(\n",
    "            [\n",
    "                \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "                \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "            ]\n",
    "        )\n",
    "        .filter((col(\"LAT\") != 0) & (col(\"LON\") != 0))\n",
    "        .select(col(\"DR_NO\"), col(\"LAT\"), col(\"LON\"))\n",
    "        .withColumn(\"geom\", ST_Point(\"LON\", \"LAT\"))\n",
    "    )\n",
    "\n",
    "    # Εντοπίζουμε τα crimes που βρίσκονται εντός κάθε COMM\n",
    "    if join_strategy == \"\":\n",
    "        non_zero_hint = non_zero\n",
    "    else:\n",
    "        non_zero_hint = non_zero.hint(join_strategy)\n",
    "        \n",
    "    start_time = time.time()    # Start timer\n",
    "    crime_comm = crime_data.join(\n",
    "        non_zero_hint,\n",
    "        ST_Within(crime_data[\"geom\"], non_zero[\"geometry\"]),\n",
    "        \"inner\"\n",
    "    )\n",
    "    crime_comm.show()\n",
    "    end_time = time.time()    # Stop timer\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Με τη μέθοδο explain() βλέπουμε πληροφορίες για την στρατηγική του join\n",
    "    if join_explain:\n",
    "        execution_plan = capture_explain_output(crime_comm)\n",
    "        \n",
    "        # Αποθηκεύουμε το χρόνο εκτ΄έλεσης και το execution plan σε ένα CSV\n",
    "        output_path = f\"s3://groups-bucket-dblab-905418150721/group37/query3_{strategy}/Second_Join\"\n",
    "        spark.createDataFrame([(strategy, elapsed_time, execution_plan)],\n",
    "                             [\"Join Strategy\", \"Elapsed Time (sec)\", \"Execution Plan\"]) \\\n",
    "                            .coalesce(1) \\\n",
    "                            .write \\\n",
    "                            .mode(\"overwrite\") \\\n",
    "                            .option(\"header\", \"true\") \\\n",
    "                            .csv(output_path)\n",
    "\n",
    "    # Μετράμε το πλήθος των εγκλημάτων κάθε COMM\n",
    "    num_of_crimes = (\n",
    "        crime_comm\n",
    "        .groupBy(crime_comm[\"COMM\"])\n",
    "        .count()\n",
    "    )\n",
    "\n",
    "    ####################################### Τελικά αποτελέσματα #######################################\n",
    "\n",
    "    # Τελικό join: Συνδυάζουμε num_of_crimes και comm_income για να \n",
    "    # υπολογίσουμε τους λόγους εισόδημα/κάτοικο και εγκλήματα/κάτοικο\n",
    "    if join_strategy == \"\":\n",
    "        num_of_crimes_hint = num_of_crimes\n",
    "    else:\n",
    "        num_of_crimes_hint = num_of_crimes.hint(join_strategy)\n",
    "    \n",
    "    start_time = time.time()   # Start timer\n",
    "    crimes_income_per_comm = (\n",
    "        comm_income\n",
    "        .join(num_of_crimes.hint(join_strategy), comm_income[\"COMM\"] == num_of_crimes[\"COMM\"], \"outer\")\n",
    "        .select(\n",
    "            comm_income[\"COMM\"],\n",
    "            (col(\"total_money\") / col(\"total_pop\")).alias(\"Annual Average Income Per Person ($)\"),\n",
    "            (col(\"count\") / col(\"total_pop\")).alias(\"Rate of Total Crimes Per Person\")\n",
    "        )\n",
    "    )\n",
    "    crimes_income_per_comm.show()\n",
    "    end_time = time.time()     # Stop timer\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "\n",
    "    # Με τη μέθοδο explain() βλέπουμε πληροφορίες για την στρατηγική του join\n",
    "    if join_explain:\n",
    "        execution_plan = capture_explain_output(crimes_income_per_comm)\n",
    "        \n",
    "        # Αποθηκεύουμε το χρόνο εκτ΄έλεσης και το execution plan σε ένα CSV\n",
    "        output_path = f\"s3://groups-bucket-dblab-905418150721/group37/query3_{strategy}/Third_Join\"\n",
    "        spark.createDataFrame([(strategy, elapsed_time, execution_plan)],\n",
    "                             [\"Join Strategy\", \"Elapsed Time (sec)\", \"Execution Plan\"]) \\\n",
    "                            .coalesce(1) \\\n",
    "                            .write \\\n",
    "                            .mode(\"overwrite\") \\\n",
    "                            .option(\"header\", \"true\") \\\n",
    "                            .csv(output_path)\n",
    "    \n",
    "        \n",
    "    # Επιστρέφουμε το dataframe με τα τελικά αποτελέσματα\n",
    "    return crimes_income_per_comm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e1d26-a794-41b1-9f33-1a6f8a48c1e8",
   "metadata": {},
   "source": [
    "<p>Τώρα, δοκιμάζουμε διάφορα join strategies</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f6118-ac07-4f65-be39-d66223358dc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strategies = [\"\", \"BROADCAST\", \"MERGE\", \"SHUFFLE_HASH\", \"SHUFFLE_REPLICATE_NL\"]\n",
    "\n",
    "for strategy in strategies:\n",
    "    result_query_3 = query_3(strategy, True)\n",
    "\n",
    "# Αποθηκέυουμε τα αποτελέσματα σε ένα CSV\n",
    "# output_path = f\"s3://groups-bucket-dblab-905418150721/group37/query3_results\"\n",
    "# result_query_3.write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d57ea85-8b51-47f6-be19-2b317ac7d5fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query 3 (αποτυχημένη προσπάθεια)\n",
    "\n",
    "<p>Προσπαθήσαμε να κάνουμε enforce τα διάφορα join strategies, αλλά δεν το καταφέραμε.\n",
    "Ο catalyst optimizer συνέχισε να αγνοεί τα hints που δίναμε.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e2963a-f888-40f7-b6cb-dbe15487b4eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "import time\n",
    "from sedona.register import SedonaRegistrator\n",
    "from pyspark.sql.functions import broadcast\n",
    "from sedona.spark import SedonaContext\n",
    "from sedona.spark import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, sum, count\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Function to capture explain() output in a string\n",
    "# --------------------------------------------------------------------------------\n",
    "def capture_explain_output(df):\n",
    "    \"\"\"\n",
    "    Calls df.explain() but captures its stdout text in a StringIO\n",
    "    and returns that string without printing to console.\n",
    "    \"\"\"\n",
    "    buffer = io.StringIO()\n",
    "    old_stdout = sys.stdout\n",
    "    try:\n",
    "        sys.stdout = buffer\n",
    "        df.explain()\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "    return buffer.getvalue()\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# The main query function\n",
    "# --------------------------------------------------------------------------------\n",
    "def query_3(join_strategy=\"\"):\n",
    "    \"\"\"\n",
    "    :param join_strategy: Enforces a specific join strategy using Spark hints.\n",
    "                         Valid values: \"BROADCAST\", \"MERGE\", \"SHUFFLE_HASH\", \"SHUFFLE_REPLICATE_NL\"\n",
    "                         If empty, no hint is used, and Spark picks the plan.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 1) Force no auto-broadcast inside this function.\n",
    "    # ------------------------------------------------------------------------\n",
    "    spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "    spark.conf.set(\"spark.sql.crossJoin.enabled\", True)\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 2) Create SedonaContext\n",
    "    # ------------------------------------------------------------------------\n",
    "    sedona = SedonaContext.create(spark)\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 3) Read Census Blocks (GEOJSON)\n",
    "    # ------------------------------------------------------------------------\n",
    "    geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "    blocks_df = (\n",
    "        sedona.read\n",
    "        .format(\"geojson\")\n",
    "        .option(\"multiLine\", \"true\")\n",
    "        .load(geojson_path)\n",
    "        .selectExpr(\"explode(features) as features\")\n",
    "        .select(\"features.*\")\n",
    "    )\n",
    "\n",
    "    # Flatten\n",
    "    flattened_df = (\n",
    "        blocks_df.select(\n",
    "            [\n",
    "                col(f\"properties.{c}\").alias(c)\n",
    "                for c in blocks_df.schema[\"properties\"].dataType.fieldNames()\n",
    "            ] + [\"geometry\"]\n",
    "        )\n",
    "        .drop(\"properties\")\n",
    "        .drop(\"type\")\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 4) Read Income CSV\n",
    "    # ------------------------------------------------------------------------\n",
    "    income_df = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .options(header=\"true\", inferSchema=\"true\", quote='\"', escape='\"')\n",
    "        .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\")\n",
    "    )\n",
    "\n",
    "    # Clean up 'Estimated Median Income' column\n",
    "    income_df = (\n",
    "        income_df\n",
    "        .withColumn(\"Estimated Median Income\",\n",
    "                    regexp_replace(col(\"Estimated Median Income\"), r\"\\$\", \"\"))\n",
    "        .withColumn(\"Estimated Median Income\",\n",
    "                    regexp_replace(col(\"Estimated Median Income\"), r\",\", \"\"))\n",
    "        .withColumn(\"Estimated Median Income\",\n",
    "                    col(\"Estimated Median Income\").cast(DecimalType(10, 2)))\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 5) Prepare data: only LA City, remove negative values\n",
    "    # ------------------------------------------------------------------------\n",
    "    non_zero = flattened_df.filter(\n",
    "        (col(\"HOUSING10\") >= 0) & \n",
    "        (col(\"POP_2010\") >= 0) & \n",
    "        (col(\"CITY\") == \"Los Angeles\")\n",
    "    )\n",
    "\n",
    "    population_agg = (\n",
    "        non_zero\n",
    "        .groupBy(\"COMM\", \"ZCTA10\")\n",
    "        .agg(\n",
    "            sum(\"HOUSING10\").alias(\"total_houses\"),\n",
    "            sum(\"POP_2010\").alias(\"total_pop\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # FIRST JOIN: population_agg vs income_df\n",
    "    # --------------------------------------------------------------------------------\n",
    "    if join_strategy:\n",
    "        income_hint = income_df.hint(join_strategy)\n",
    "    else:\n",
    "        income_hint = income_df\n",
    "\n",
    "    # Build the join DataFrame\n",
    "    join_df_1 = (\n",
    "        population_agg\n",
    "        .join(income_hint, population_agg[\"ZCTA10\"] == col(\"Zip Code\"), \"inner\")\n",
    "        .select(\n",
    "            population_agg.ZCTA10,\n",
    "            population_agg.COMM,\n",
    "            population_agg.total_pop,\n",
    "            (population_agg.total_houses * col(\"Estimated Median Income\")).alias(\"total_money\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Measure time by forcing execution with .show()\n",
    "    start_time = time.time()\n",
    "    join_df_1.show(5, False)   # triggers actual job\n",
    "    end_time = time.time()\n",
    "    elapsed_1 = end_time - start_time\n",
    "\n",
    "    # Capture plan (without printing)\n",
    "    explain_text_1 = capture_explain_output(join_df_1)\n",
    "\n",
    "    # Write plan/time to S3\n",
    "    first_join_output = [(\"Execution Plan\", explain_text_1),\n",
    "                         (\"Elapsed Time (seconds)\", str(elapsed_1))]\n",
    "    first_join_df = spark.createDataFrame(first_join_output, [\"Description\", \"Details\"])\n",
    "    first_join_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\n",
    "        f\"s3://groups-bucket-dblab-905418150721/group37/query3_{join_strategy}/First_Join\"\n",
    "    )\n",
    "\n",
    "    # Aggregate partial result by COMM\n",
    "    comm_income = (\n",
    "        join_df_1\n",
    "        .groupBy(\"COMM\")\n",
    "        .agg(\n",
    "            sum(\"total_money\").alias(\"total_money\"),\n",
    "            sum(\"total_pop\").alias(\"total_pop\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # SECOND JOIN: crime_data vs non_zero (spatial join via ST_Within)\n",
    "    # --------------------------------------------------------------------------------\n",
    "    crime_data = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .options(header=\"true\", inferSchema=\"true\", quote='\"', escape='\"')\n",
    "        .load([\n",
    "            \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "            \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "        ])\n",
    "        .filter((col(\"LAT\") != 0) & (col(\"LON\") != 0))\n",
    "        .select(col(\"DR_NO\"), col(\"LAT\"), col(\"LON\"))\n",
    "        .withColumn(\"geom\", ST_Point(\"LON\", \"LAT\"))\n",
    "    )\n",
    "\n",
    "   \n",
    " \n",
    "    if join_strategy == \"SHUFFLE_HASH\":\n",
    "        spark.conf.set(\"spark.sql.join.preferShuffledHashJoin\", True)\n",
    "        non_zero_hint = non_zero.hint(join_strategy)\n",
    "\n",
    "   \n",
    "    elif join_strategy == \"MERGE\":\n",
    "        spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "        spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", True)\n",
    "        spark.conf.set(\"spark.sql.join.preferShuffledHashJoin\", False)\n",
    "        spark.conf.set(\"spark.sql.rangeJoin.enabled\", False)  # Disable Range Join (if supported)\n",
    "        non_zero_hint = non_zero.hint(join_strategy)\n",
    "    \n",
    "    elif join_strategy == \"\":\n",
    "        non_zero_hint = non_zero\n",
    "        \n",
    "    else:\n",
    "        non_zero_hint = non_zero.hint(join_strategy)\n",
    "\n",
    "\n",
    "    start_time = time.time()    \n",
    "    join_df_2 = crime_data.join(\n",
    "        non_zero_hint,\n",
    "        ST_Within(crime_data[\"geom\"], non_zero_hint[\"geometry\"]),\n",
    "        \"inner\"\n",
    "    )\n",
    "\n",
    "   \n",
    "    join_df_2.show(5, False)  # triggers execution\n",
    "    end_time = time.time()\n",
    "    elapsed_2 = end_time - start_time\n",
    "\n",
    "    # Capture plan\n",
    "    explain_text_2 = capture_explain_output(join_df_2)\n",
    "\n",
    "    # Write plan/time to S3\n",
    "    second_join_output = [(\"Execution Plan\", explain_text_2),\n",
    "                          (\"Elapsed Time (seconds)\", str(elapsed_2))]\n",
    "    second_join_df = spark.createDataFrame(second_join_output, [\"Description\", \"Details\"])\n",
    "    second_join_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\n",
    "        f\"s3://groups-bucket-dblab-905418150721/group37/query3_{join_strategy}/Second_Join\"\n",
    "    )\n",
    "\n",
    "    num_of_crimes = join_df_2.groupBy(\"COMM\").count()\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # THIRD JOIN: comm_income vs num_of_crimes\n",
    "    # --------------------------------------------------------------------------------\n",
    "    start_time = time.time()\n",
    "\n",
    "# Ensure `crimes_hint` is always assigned\n",
    "    if join_strategy == \"BROADCAST\":\n",
    "        crimes_hint = broadcast(num_of_crimes)  # Apply broadcast hint\n",
    "   \n",
    "    elif join_strategy != \"\":\n",
    "        crimes_hint = num_of_crimes.hint(join_strategy)\n",
    "    \n",
    "    else:\n",
    "        crimes_hint = num_of_crimes  # Default case when no join strategy is given\n",
    "\n",
    "# Perform the join\n",
    "    join_df_3 = (\n",
    "    comm_income\n",
    "    .join(crimes_hint, \"COMM\", \"outer\")\n",
    "    .select(\n",
    "        comm_income[\"COMM\"],\n",
    "        (col(\"total_money\") / col(\"total_pop\")).alias(\"Annual Average Income Per Person ($)\"),\n",
    "        (col(\"count\") / col(\"total_pop\")).alias(\"Rate of Total Crimes Per Person\")\n",
    "    )\n",
    ")\n",
    "\n",
    "    join_df_3.show(5, False)\n",
    "    end_time = time.time()\n",
    "    elapsed_3 = end_time - start_time\n",
    "\n",
    "# Capture plan\n",
    "    explain_text_3 = capture_explain_output(join_df_3)\n",
    "\n",
    "# Write plan/time to S3\n",
    "    third_join_output = [(\"Execution Plan\", explain_text_3),\n",
    "                     (\"Elapsed Time (seconds)\", str(elapsed_3))]\n",
    "    third_join_df = spark.createDataFrame(third_join_output, [\"Description\", \"Details\"])\n",
    "    third_join_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\n",
    "    f\"s3://groups-bucket-dblab-905418150721/group37/query3_{join_strategy}/Third_Join\"\n",
    ")\n",
    "\n",
    "# Return the final result DataFrame\n",
    "    return join_df_3\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 4) Example Usage: run across multiple strategies\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "strategies = [\"BROADCAST\", \"MERGE\", \"SHUFFLE_HASH\", \"SHUFFLE_REPLICATE_NL\"]\n",
    "for strategy in strategies:\n",
    "        print(f\"\\n>>> Running with strategy = {strategy}\")\n",
    "        final_df = query_3(strategy)\n",
    "        print(\">>> Final Results (first 10 rows):\")\n",
    "        final_df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45444618-3cad-4fa7-afd9-447626d23e27",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query 4 με χρήση DataFrame API\n",
    "\n",
    "<p>Πρέπει να εκτελέσουμε πρώτα το query 3, γιατί χρησιμοποιούμε τα αποτελέσματά του</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fa16bb-6431-4a74-a80e-fc24fa0ec8ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Access configuration\n",
    "conf = spark.sparkContext.getConf()\n",
    "\n",
    "exec_instances = conf.get(\"spark.executor.instances\")\n",
    "exec_mem = conf.get(\"spark.executor.memory\")\n",
    "exec_cores = conf.get(\"spark.executor.cores\")\n",
    "\n",
    "# Print relevant executor settings\n",
    "print(\"Executor Instances:\", exec_instances)\n",
    "print(\"Executor Memory:\", exec_mem)\n",
    "print(\"Executor Cores:\", exec_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdaf1a9-d8e5-46da-b9bf-b93361f5f856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, desc, asc, lit\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "# Start timing\n",
    "start = time.time()\n",
    "\n",
    "# Διαβάζουμε τα δεδομένα με τα εγκλήματα 2010 - 2019\n",
    "# Αφαιρούμε τα εγκλήματα που αναφέρονται στο Null Island (0, 0) και\n",
    "# τα εγκλήματα που δεν αναφέρουν το φυλετικό προφιλ του θύματος (Null)\n",
    "# Κρατάμε μόνο τα εγκλήματα που καταγράφηκαν (όχι συνέβησαν) το 2015\n",
    "crime_data = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .options(header=\"true\", inferSchema=\"true\", quote='\"', escape='\"')\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\")\n",
    "    .filter(\n",
    "        (col(\"LAT\") != 0) &\n",
    "        (col(\"LON\") != 0) &\n",
    "        (col(\"Vict Descent\").isNotNull()) &\n",
    "        (col(\"Date Rptd\").contains(\"/2015\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Κρατάμε μόνο τις στήλες DR_NO, Vict Descent, LAT και LON του crime_data για απλότητα\n",
    "# Δημιουργούμε μία στήλη geometry type από τις συντεταγμένες (lat, lon)\n",
    "crime_data_simplified = (\n",
    "    crime_data.select(\n",
    "        col(\"DR_NO\"),\n",
    "        col(\"Vict Descent\"),\n",
    "        col(\"LAT\"),\n",
    "        col(\"LON\")\n",
    "    )\n",
    "    .withColumn(\"geom\", ST_Point(\"LON\", \"LAT\"))\n",
    ")\n",
    "\n",
    "# Διαβάζουμε το σύνολο δεδομενων Race and Ethnicity codes\n",
    "re_codes = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .options(header=\"true\", inferSchema=\"true\")\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\")\n",
    ")\n",
    "\n",
    "# Διαβάζουμε τα δεδομένα της απογραφής πληθυσμού (geojson)\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = (\n",
    "    sedona.read.format(\"geojson\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(geojson_path)\n",
    "    .selectExpr(\"explode(features) as features\")\n",
    "    .select(\"features.*\")\n",
    ")\n",
    "# Formatting magic - Flattening\n",
    "flattened_df = (\n",
    "    blocks_df.select(\n",
    "        [\n",
    "            col(f\"properties.{c}\").alias(c)\n",
    "            for c in blocks_df.schema[\"properties\"].dataType.fieldNames()\n",
    "        ] + [\"geometry\"]\n",
    "    )\n",
    "    .drop(\"properties\")\n",
    "    .drop(\"type\")\n",
    ")\n",
    "\n",
    "# Κρατάμε τα rows που βρίσκονται στην πόλη του Los Angeles.\n",
    "comm_in_la = flattened_df.filter(col(\"CITY\") == \"Los Angeles\")\n",
    "\n",
    "# Θα χρησιμοποιήσουμε τα αποτελέσματα του προηγούμενου ερωτη΄ματος,\n",
    "# τα οποία βρίσκονται στο dataframe result_query_3. Από αυτά, θα\n",
    "# κρατήσουμε τα top 3 rows με το υψηλότερο μέσο εισόδημα. Χρειαζόμαστε\n",
    "# μόνο τις στήλες COMM και Annual Average Income Per Person ($)\n",
    "three_highest_paid = (\n",
    "    result_query_3\n",
    "    .select(\n",
    "        col(\"COMM\").alias(\"community\"),\n",
    "        col(\"Annual Average Income Per Person ($)\")\n",
    "    )\n",
    "    .orderBy(desc(\"Annual Average Income Per Person ($)\"))\n",
    "    .limit(3)\n",
    "    .withColumn(\"rank_label\", lit(\"highest\"))  # Add label\n",
    ")\n",
    "\n",
    "# Ομοίως, φτιάχνουμε και ένα dataframe με τις top 3 περιοχές με το\n",
    "# χαμηλότερο μέσο εισόδημα\n",
    "three_lowest_paid = (\n",
    "    result_query_3\n",
    "    .select(\n",
    "        col(\"COMM\").alias(\"community\"),\n",
    "        col(\"Annual Average Income Per Person ($)\")\n",
    "    )\n",
    "    .orderBy(asc(\"Annual Average Income Per Person ($)\"))\n",
    "    .limit(3)\n",
    "    .withColumn(\"rank_label\", lit(\"lowest\"))   # Add label\n",
    ")\n",
    "\n",
    "# Ενώνουμε τα δύο dataframes three_highest_paid και three_lowest_paid\n",
    "# σε ένα ενιαίο dataframe\n",
    "top_3_comms_union = three_highest_paid.union(three_lowest_paid)\n",
    "\n",
    "# Πραγματοποιούμε ένα join μεταξύ του top_3_comms_union και comm_in_la\n",
    "# για να προσθέσουμε τις συντεταγμένες κάθε περιοχής του top_3_comms_union\n",
    "top_3_comms_coords = (\n",
    "    comm_in_la.join(\n",
    "        top_3_comms_union,\n",
    "        comm_in_la[\"COMM\"] == top_3_comms_union[\"community\"],\n",
    "        \"inner\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"COMM\"),\n",
    "        col(\"geometry\"),\n",
    "        col(\"rank_label\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Βρίσκουμε ποιες αστυνομικές υποθέσεις αναφέρονται σε σημεία εντός των 3 αυτών περιοχών\n",
    "# και κρατάμε μόνο τις στήλες DR_NO, Vict Descent και rank_label.\n",
    "top_3_comms_crimes = (\n",
    "    crime_data_simplified.join(\n",
    "        top_3_comms_coords,\n",
    "        ST_Within(crime_data_simplified[\"geom\"], top_3_comms_coords[\"geometry\"]),\n",
    "        \"inner\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"DR_NO\"),\n",
    "        col(\"Vict Descent\"),\n",
    "        col(\"rank_label\")  # χρειαζόμαστε την στήλη για να ξεχωρίσουμε\n",
    "    )                      # αργ΄ότερα τα αποτελέσματα σε δύο dataframes\n",
    ")\n",
    "\n",
    "# Ομαδοποιούμε κατά (rank_label, Vict Descent) και\n",
    "# βρίσκουμε, για κάθε φυλή, πόσα θύματα καταγράφηκαν\n",
    "vict_count_per_race = (\n",
    "    top_3_comms_crimes\n",
    "    .groupBy(col(\"rank_label\"), col(\"Vict Descent\"))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# Join race-ethnicity codes\n",
    "vict_race_count = (\n",
    "    vict_count_per_race\n",
    "    .join(\n",
    "        re_codes,\n",
    "        vict_count_per_race[\"Vict Descent\"] == re_codes[\"Vict Descent\"],\n",
    "        \"inner\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"rank_label\"),\n",
    "        col(\"Vict Descent Full\").alias(\"Victim Descent\"),\n",
    "        col(\"count\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Το end θα περιέχει τη χρονική στιγμή τέλους. Προς το παρόν, όμως, αποθη-\n",
    "# κεύουμε σε αυτό τη χρονική στιγμή μόλις πριν το for loop.\n",
    "end = time.time()\n",
    "\n",
    "# Δημιουργούμε δύο ξεχωριστά dataframes: ένα για τις περιοχές με το υψηλότερο\n",
    "# “highest” εισόδημα και ένα για τις περιοχές με το χαμηλότερο “lowest” εισόδημα\n",
    "for label in [\"highest\", \"lowest\"]:\n",
    "    start_loop = time.time()        # η χρονική στιγμή έναρξης του iteration\n",
    "    df_label = vict_race_count.filter(col(\"rank_label\") == label).orderBy(desc(\"count\"))\n",
    "    df_label = df_label.drop('rank_label')\n",
    "    print(f\"Victim Descent in top 3 {label} paid communities in Los Angeles:\")\n",
    "   \n",
    "    df_label.show()\n",
    "    \n",
    "    # ενημερώνουμε τη μεταβλητή end\n",
    "    end = end + (time.time() - start_loop)\n",
    "\n",
    "    output_path = f\"s3://groups-bucket-dblab-905418150721/group37/query4_{label}_paid\"\n",
    "    (\n",
    "        df_label\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .csv(output_path)\n",
    "    )\n",
    "\n",
    "# Αποθηκεύουμε το χρόνο εκτέλεσης του query σε ένα CSV αρχείο\n",
    "\n",
    "elapsed_time = end - start\n",
    "output_path = f\"s3://groups-bucket-dblab-905418150721/group37/query4_time_{exec_instances}executors_{exec_cores}cores_{exec_mem}mem\"\n",
    "\n",
    "spark.createDataFrame([(exec_instances, exec_cores, exec_mem, elapsed_time)],\n",
    "                      [\n",
    "                          \"Executor Instances\", \"Executor Cores\",\n",
    "                           \"Executor Memory\", \"Elapsed Time (sec)\"\n",
    "                      ]) \\\n",
    "                    .coalesce(1) \\\n",
    "                    .write \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .option(\"header\", \"true\") \\\n",
    "                    .csv(output_path)\n",
    "\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cad7d29-5206-4b6d-8271-25a2fd869070",
   "metadata": {},
   "source": [
    "### Query 5 με χρήση DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796874fc-36cf-43cb-8a03-d09863912178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Access configuration\n",
    "conf = spark.sparkContext.getConf()\n",
    "\n",
    "exec_instances = conf.get(\"spark.executor.instances\")\n",
    "exec_mem = conf.get(\"spark.executor.memory\")\n",
    "exec_cores = conf.get(\"spark.executor.cores\")\n",
    "\n",
    "print(exec_instances)\n",
    "print(exec_mem)\n",
    "print(exec_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d3c924-0622-47d4-b27b-7e61173939d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, desc, asc, lit, expr, row_number, broadcast, avg\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# Create Sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Load and filter crime data\n",
    "crime_data1 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .options(header=\"true\", inferSchema=\"true\", quote='\"', escape='\"')\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\")\n",
    "    .filter((col(\"LAT\") != 0) & (col(\"LON\") != 0))\n",
    ")\n",
    "\n",
    "crime_data2 = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .options(header=\"true\", inferSchema=\"true\", quote='\"', escape='\"')\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\")\n",
    "    .filter((col(\"LAT\") != 0) & (col(\"LON\") != 0))\n",
    ")\n",
    "\n",
    "# Combine the datasets\n",
    "crime_data = crime_data1.union(crime_data2)\n",
    "\n",
    "# Keep needed columns and create points\n",
    "crime_data_simplified = (\n",
    "    crime_data\n",
    "    .select(col(\"LAT\"), col(\"LON\"), col(\"DR_NO\"))\n",
    "    .withColumn(\"Crime_Location_Point\", ST_Point(\"LON\", \"LAT\"))\n",
    "    .drop(\"LON\")\n",
    "    .drop(\"LAT\")\n",
    ")\n",
    "\n",
    "# Load the Police Stations data\n",
    "police_stations = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .options(header=\"true\", inferSchema=\"true\", quote='\"', escape='\"')\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\")\n",
    ")\n",
    "\n",
    "# Simplify police stations data\n",
    "police_stations_simplified = (\n",
    "    police_stations\n",
    "    .select(\n",
    "        col(\"X\").alias(\"LON\"),\n",
    "        col(\"Y\").alias(\"LAT\"),\n",
    "        col(\"DIVISION\").alias(\"division\")\n",
    "    )\n",
    "    .withColumn(\"Police_Station_Location_Point\", ST_Point(\"LON\", \"LAT\"))\n",
    "    .drop(\"LON\")\n",
    "    .drop(\"LAT\")\n",
    ")\n",
    "\n",
    "# Perform cross join with broadcast\n",
    "cross_df = crime_data_simplified.crossJoin(broadcast(police_stations_simplified))\n",
    "\n",
    "# Compute distance in km\n",
    "cross_df = cross_df.withColumn(\n",
    "    \"distance_km\",\n",
    "    expr(\"ST_DistanceSphere(Crime_Location_Point, Police_Station_Location_Point) / 1000.0\")\n",
    ")\n",
    "\n",
    "# For each crime pick the station with the minimal distance\n",
    "window = Window.partitionBy(\"DR_NO\").orderBy(col(\"distance_km\").asc())\n",
    "ranked_df = cross_df.withColumn(\"rn\", row_number().over(window))\n",
    "\n",
    "assigned_stations = ranked_df.filter(col(\"rn\") == 1)\n",
    "\n",
    "# Group by station: average distance and count (#)\n",
    "query_5_df = (\n",
    "    assigned_stations\n",
    "    .groupBy(\"division\")\n",
    "    .agg(\n",
    "        avg(\"distance_km\").alias(\"average_distance\"),\n",
    "        count(\"*\").alias(\"#\")\n",
    "    )\n",
    "    .orderBy(desc(\"#\"))\n",
    ")\n",
    "\n",
    "\n",
    "query_5_df.show()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "elapsed_time = end - start\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time} sec\")\n",
    "\n",
    "\n",
    "# Define output path\n",
    "output_path = f\"s3://groups-bucket-dblab-905418150721/group37/query5_results_{exec_instances}executors_{exec_cores}cores_{exec_mem}mem\"\n",
    "\n",
    "output_path_2 = f\"s3://groups-bucket-dblab-905418150721/group37/query5_results\"\n",
    "\n",
    "query_5_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(f\"{output_path_2}\")\n",
    "\n",
    "# Save results\n",
    "spark.createDataFrame([(exec_instances, exec_cores, exec_mem, elapsed_time)],\n",
    "                      [\n",
    "                          \"Executor Instances\", \"Executor Cores\",\n",
    "                          \"Executor Memory\", \"Elapsed Time (sec)\"\n",
    "                      ]) \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "python3",
   "version": "3.11.2"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
